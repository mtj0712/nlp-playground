{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4e41f10",
   "metadata": {},
   "source": [
    "# Add-One Smoothing\n",
    "\n",
    "Here, I take some English corpora - the King James Version of the Bible, The 1662 Book of Common Prayer, The Complete Works of Shakespeare, and the Universal Declaration of Human Rights - and apply add-one smoothing to generate n-gram models. For simplicity, all words will be in lowercase and the models will be case-insensitive.\n",
    "\n",
    "Add-one smoothing is a method of computing the probability of a word in a n-gram model in such a way that the sequences that never appear in the corpus do not get zero probability.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "p &= \\frac{c + 1}{n + v} \\\\\n",
    "c &= \\textrm{count of the n-gram} \\\\\n",
    "n &= \\textrm{count of the history (the n-gram excluding the last word)} \\\\\n",
    "v &= \\textrm{size of the vocabulary}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "For each model, I compute its cross-entropy and perplexity and develop a simple sentence generator. After that, I analyze the effectiveness of each corpus at training n-gram models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77e14b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3ac45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "\n",
    "import pygtrie\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "\n",
    "from reader import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c4db99-94d5-4cf9-a6bd-e9dfb5e484b8",
   "metadata": {},
   "source": [
    "`punc_pattern` will help us separate the punctuations from actual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2ab466-d6e6-4393-aee1-4c94cfccd58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "punc_pattern = re.compile('[‘’“”!\\\"#$%&\\'()*+,\\\\-./:;<=>?@[\\\\\\\\\\\\]^_`{|}~]')\n",
    "end_mark_set = {'!', '.', '?'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f31ead1-b53d-4ed7-8e82-83d890ba746e",
   "metadata": {},
   "source": [
    "## Early Modern English\n",
    "\n",
    "First, I build n-gram models with the King James Version of the Bible and The 1662 Book of Common Prayer. This will give us a language model for Early Modern English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63208359",
   "metadata": {},
   "outputs": [],
   "source": [
    "kjv = KJVReader()\n",
    "bcp = BCPReader()\n",
    "shakespeare = ShakespeareReader()\n",
    "eme_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4ec3f9-8d60-49a3-822c-7e8b5636b076",
   "metadata": {},
   "source": [
    "Before building the models, I parse the text into a list of words and punctuations. This will be convenient for building the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59325889",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "while not bcp.is_eof():\n",
    "    units = bcp.read_sentence().lower().split()\n",
    "    for u in units:\n",
    "        while u:\n",
    "            match = punc_pattern.search(u)\n",
    "            if match:\n",
    "                i = match.start()\n",
    "                if i != 0:\n",
    "                    first_word = u[:i]\n",
    "                    eme_list.append(first_word)\n",
    "                \n",
    "                if u[i:i+2] == '&c':\n",
    "                    punc = u[i:i+2]\n",
    "                    u = u[i+2:]\n",
    "                else:\n",
    "                    punc = u[i]\n",
    "                    u = u[i+1:]\n",
    "                eme_list.append(punc)\n",
    "            else:\n",
    "                eme_list.append(u)\n",
    "                break\n",
    "\n",
    "while not kjv.is_eof():\n",
    "    units = kjv.read_sentence().lower().split()\n",
    "    for u in units:\n",
    "        while u:\n",
    "            match = punc_pattern.search(u)\n",
    "            if match:\n",
    "                i = match.start()\n",
    "                if i != 0:\n",
    "                    first_word = u[:i]\n",
    "                    eme_list.append(first_word)\n",
    "                punc = u[i]\n",
    "                u = u[i+1:]\n",
    "                eme_list.append(punc)\n",
    "            else:\n",
    "                eme_list.append(u)\n",
    "                break\n",
    "\n",
    "while not shakespeare.is_eof():\n",
    "    units = shakespeare.read_sentence().lower().split()\n",
    "    for u in units:\n",
    "        while u:\n",
    "            match = punc_pattern.search(u)\n",
    "            if match:\n",
    "                i = match.start()\n",
    "                if i != 0:\n",
    "                    first_word = u[:i]\n",
    "                    eme_list.append(first_word)\n",
    "                if u[i:i+2] == '&c':\n",
    "                    punc = u[i:i+2]\n",
    "                    u = u[i+2:]\n",
    "                else:\n",
    "                    punc = u[i]\n",
    "                    u = u[i+1:]\n",
    "                eme_list.append(punc)\n",
    "            else:\n",
    "                eme_list.append(u)\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089cbe85-7c32-4396-b2a6-a88d36cfbc3c",
   "metadata": {},
   "source": [
    "First, I build the unigram model without add-one smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b3ef28-61e0-4eec-945c-cd08c6dd4fa7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Unigram Model\n",
    "\n",
    "eme_trie = pygtrie.StringTrie()\n",
    "eme_v = 0 # size of the vocabulary\n",
    "eme_wordlist = []\n",
    "\n",
    "for w in eme_list:\n",
    "    try:\n",
    "        eme_trie[w] += 1\n",
    "    except KeyError:\n",
    "        eme_trie[w] = 1\n",
    "        eme_wordlist.append(w)\n",
    "        eme_v += 1\n",
    "\n",
    "print('Size of the vocabulary:', eme_v)\n",
    "print('Count of all words:', len(eme_list))\n",
    "\n",
    "eme_unigram_H = 0 # cross entropy\n",
    "\n",
    "for w in eme_list:\n",
    "    p = eme_trie[w] / len(eme_list)\n",
    "    eme_unigram_H += math.log2(p)\n",
    "eme_unigram_H /= -len(eme_list)\n",
    "\n",
    "print('Cross entropy:', eme_unigram_H)\n",
    "print('Perplexity:', 2 ** eme_unigram_H)\n",
    "print()\n",
    "\n",
    "eme_unigram_list = sorted(eme_trie.items(), key=lambda t : t[1], reverse=True)[:10]\n",
    "for pair in eme_unigram_list:\n",
    "    p = pair[1] / len(eme_list)\n",
    "    print('Word:', pair[0], '| Probability:', p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a0cffc-2c0d-4db8-8710-29f41506e72d",
   "metadata": {},
   "source": [
    "The cross entropy and perplexity of the model is extremely high. This is expected, since a unigram model is far from sufficient in representing an actual language. As expected, the most probable words are some common punctuations and grammatical words, such as articles, prepositions, and pronouns.\n",
    "\n",
    "Next, I build 2~5-gram models. Again, I do not apply add-one smoothing. This time, I do not print out the probabilities for each n-gram, since it would be too lengthy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2840c7-d8b0-4fea-8146-babc18e2c5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2~5-gram Model\n",
    "\n",
    "for n in range(2, 6):\n",
    "    print('========== ' + ('bi' if n == 2 else ('tri' if n == 3 else f'{n}-')) + 'gram ==========')\n",
    "    print()\n",
    "    \n",
    "    ngram = [''] * n\n",
    "    for w in eme_list:\n",
    "        ngram[-1] = w\n",
    "        try:\n",
    "            eme_trie['/'.join(ngram)] += 1\n",
    "        except KeyError:\n",
    "            eme_trie['/'.join(ngram)] = 1\n",
    "    \n",
    "        if w in end_mark_set:\n",
    "            ngram[:-1] = [''] * (n - 1)\n",
    "        else:\n",
    "            ngram[:-1] = ngram[1:]\n",
    "    \n",
    "    eme_ngram_H = 0 # cross entropy\n",
    "    \n",
    "    ngram = [''] * n\n",
    "    for w in eme_list:\n",
    "        ngram[-1] = w\n",
    "        if ngram[-2] == '':\n",
    "            history_count = eme_trie['.'] + eme_trie['?'] + eme_trie['!']\n",
    "        else:\n",
    "            history_count = eme_trie['/'.join(ngram[:-1])]\n",
    "        p = eme_trie['/'.join(ngram)] / history_count\n",
    "        eme_ngram_H += math.log2(p)\n",
    "        \n",
    "        if w in end_mark_set:\n",
    "            ngram[:-1] = [''] * (n - 1)\n",
    "        else:\n",
    "            ngram[:-1] = ngram[1:]\n",
    "    eme_ngram_H /= -len(eme_list)\n",
    "    \n",
    "    print('Cross entropy:', eme_ngram_H)\n",
    "    print('Perplexity:', 2 ** eme_ngram_H)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8df3fb-3adf-44a6-bb26-749b55d50637",
   "metadata": {},
   "source": [
    "As expected, as the order of the n-gram model increases, the perplexity of the language model decreases. Now, I try the same while applying add-one smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6021d43b-43e9-4fc0-ac98-5be1754aed4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('========== unigram ==========')\n",
    "print()\n",
    "\n",
    "eme_unigram_H = 0 # cross entropy\n",
    "\n",
    "for w in eme_list:\n",
    "    p = (eme_trie[w] + 1) / (len(eme_list) + eme_v)\n",
    "    eme_unigram_H += math.log2(p)\n",
    "eme_unigram_H /= -len(eme_list)\n",
    "\n",
    "print('Cross entropy:', eme_unigram_H)\n",
    "print('Perplexity:', 2 ** eme_unigram_H)\n",
    "print()\n",
    "\n",
    "for n in range(2, 6):\n",
    "    print('========== ' + ('bi' if n == 2 else ('tri' if n == 3 else f'{n}-')) + 'gram ==========')\n",
    "    print()\n",
    "    \n",
    "    eme_ngram_H = 0 # cross entropy\n",
    "    \n",
    "    ngram = [''] * n\n",
    "    for w in eme_list:\n",
    "        ngram[-1] = w\n",
    "        if ngram[-2] == '':\n",
    "            history_count = eme_trie['.'] + eme_trie['?'] + eme_trie['!']\n",
    "        else:\n",
    "            history_count = eme_trie['/'.join(ngram[:-1])]\n",
    "        p = (eme_trie['/'.join(ngram)] + 1) / (history_count + eme_v)\n",
    "        eme_ngram_H += math.log2(p)\n",
    "        \n",
    "        if w in end_mark_set:\n",
    "            ngram[:-1] = [''] * (n - 1)\n",
    "        else:\n",
    "            ngram[:-1] = ngram[1:]\n",
    "    eme_ngram_H /= -len(eme_list)\n",
    "    \n",
    "    print('Cross entropy:', eme_ngram_H)\n",
    "    print('Perplexity:', 2 ** eme_ngram_H)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655734e3-f8e6-4580-b110-b2756d99df23",
   "metadata": {},
   "source": [
    "When add-one smoothing is applied, contrary to our expectation, the perplexity of the language model increases as the order of the n-gram model increases. This might be because at higher orders of the n-gram model, the count of the history before each word ($n$) is smaller, and this causes the size of the vocabulary ($v$) to comprise a bigger portion in the denominator of the probability fraction.\n",
    "\n",
    "It seems that add-one smoothing is only useful for dealing with new texts, and not for computing the cross-entropy or the perplexity of a language model with the very corpus it was trained on.\n",
    "\n",
    "Below, I implement a sentence generator with 2~5-gram models developed above. New word is generated by randomly choosing from the corpus vocabulary with the probability calculated with add-one smoothing. Whenever the history of an n-gram cannot be found in the corpus, I reduce the order of the n-gram by $1$. After that, I try to increase the order back to `n`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24778b3e-996f-4010-a736-ecae5a6c27b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(2, 6):\n",
    "    print('========== ' + ('bi' if n == 2 else ('tri' if n == 3 else f'{n}-')) + 'gram ==========')\n",
    "    print()\n",
    "    \n",
    "    ngram = [''] * n\n",
    "    sentence_len = 0\n",
    "    start = 0 # for finding the starting point of history\n",
    "    \n",
    "    while ngram[-1] not in end_mark_set:\n",
    "        if 200 <= sentence_len:\n",
    "            print(' <<< The sentence being generated has exceeded 200 tokens. Sentence finishing failed. >>>')\n",
    "            break\n",
    "        \n",
    "        ngram[:-1] = ngram[1:]\n",
    "        \n",
    "        if ngram[-2] == '':\n",
    "            history = '/' * (n - 2)\n",
    "            history_count = eme_trie['.'] + eme_trie['?'] + eme_trie['!']\n",
    "        else:\n",
    "            if 0 < start:\n",
    "                start -= 1\n",
    "            \n",
    "            for i in range(start, n - 1):\n",
    "                history = '/'.join(ngram[i:-1])\n",
    "                try:\n",
    "                    history_count = eme_trie[history]\n",
    "                    start = i\n",
    "                    break\n",
    "                except KeyError:\n",
    "                    pass\n",
    "\n",
    "        p = np.zeros(eme_v)\n",
    "        for i in range(eme_v):\n",
    "            try:\n",
    "                count = eme_trie[history + '/' + eme_wordlist[i]]\n",
    "            except KeyError:\n",
    "                count = 0\n",
    "            p[i] = (count + 1) / (history_count + eme_v)\n",
    "        \n",
    "        ngram[-1] = eme_wordlist[random.choice(eme_v, p=p)]\n",
    "\n",
    "        if sentence_len == 0 or punc_pattern.fullmatch(ngram[-1]) or ngram[-2] == '-' or ngram[-2:] == ['’', 's']:\n",
    "            output = ngram[-1]\n",
    "        else:\n",
    "            output = ' ' + ngram[-1]\n",
    "        print(output, end='')\n",
    "\n",
    "        sentence_len += 1\n",
    "    \n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590b9162-49a1-432a-97df-c54a61198c84",
   "metadata": {},
   "source": [
    "As anyone can tell from the outputs above, the generated sentences do not make sense and are too long. This might be because add-one smoothing gives significant probability to words that are otherwise extremely unlikely to appear.\n",
    "\n",
    "Now, I will once again try generating sentences, but this time add-one smoothing will not be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d63b323-b1b0-473b-9d07-7b1c5beee367",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(2, 6):\n",
    "    print('========== ' + ('bi' if n == 2 else ('tri' if n == 3 else f'{n}-')) + 'gram ==========')\n",
    "    print()\n",
    "    \n",
    "    ngram = [''] * n\n",
    "    sentence_len = 0\n",
    "    \n",
    "    while ngram[-1] not in end_mark_set:\n",
    "        if 200 <= sentence_len:\n",
    "            print(' <<< The sentence being generated has exceeded 200 tokens. Sentence finishing failed. >>>')\n",
    "            break\n",
    "        \n",
    "        ngram[:-1] = ngram[1:]\n",
    "        \n",
    "        if ngram[-2] == '':\n",
    "            history = '/' * (n - 2)\n",
    "            history_count = eme_trie['.'] + eme_trie['?'] + eme_trie['!']\n",
    "        else:\n",
    "            history = '/'.join(ngram[:-1])\n",
    "            history_count = eme_trie[history]\n",
    "\n",
    "        p = np.zeros(eme_v)\n",
    "        for i in range(eme_v):\n",
    "            try:\n",
    "                p[i] = eme_trie[history + '/' + eme_wordlist[i]] / history_count\n",
    "            except KeyError:\n",
    "                pass\n",
    "        \n",
    "        ngram[-1] = eme_wordlist[random.choice(eme_v, p=p)]\n",
    "\n",
    "        if sentence_len == 0 or punc_pattern.fullmatch(ngram[-1]) or ngram[-2] == '-' or ngram[-2:] == ['’', 's']:\n",
    "            output = ngram[-1]\n",
    "        else:\n",
    "            output = ' ' + ngram[-1]\n",
    "        print(output, end='')\n",
    "\n",
    "        sentence_len += 1\n",
    "    \n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a18fb8-df80-4016-bef6-57d96a249fb4",
   "metadata": {},
   "source": [
    "The sentences are much shorter and somewhat recognizable. The meaning of the sentence generated by the 5-gram model makes more sense than that of the sentence generated by the bigram model.\n",
    "\n",
    "This time, each new word will be the most probable word at that position. If there are multiple words with equal probability that are most probable, one of those words will be chosen randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71d1b12-759a-4df1-9c68-6b9855f1ab34",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(2, 6):\n",
    "    print('========== ' + ('bi' if n == 2 else ('tri' if n == 3 else f'{n}-')) + 'gram ==========')\n",
    "    print()\n",
    "    \n",
    "    ngram = [''] * n\n",
    "    sentence_len = 0\n",
    "    \n",
    "    while ngram[-1] not in end_mark_set:\n",
    "        if 200 <= sentence_len:\n",
    "            print(' <<< The sentence being generated has exceeded 200 tokens. Sentence finishing failed. >>>')\n",
    "            break\n",
    "        \n",
    "        ngram[:-1] = ngram[1:]\n",
    "        \n",
    "        if ngram[-2] == '':\n",
    "            history = '/' * (n - 2)\n",
    "            history_count = eme_trie['.'] + eme_trie['?'] + eme_trie['!']\n",
    "        else:\n",
    "            if 0 < start:\n",
    "                start -= 1\n",
    "            \n",
    "            for i in range(start, n - 1):\n",
    "                history = '/'.join(ngram[i:-1])\n",
    "                try:\n",
    "                    history_count = eme_trie[history]\n",
    "                    start = i\n",
    "                    break\n",
    "                except KeyError:\n",
    "                    pass\n",
    "\n",
    "        p = np.zeros(eme_v)\n",
    "        for i in range(eme_v):\n",
    "            try:\n",
    "                p[i] = eme_trie[history + '/' + eme_wordlist[i]] / history_count\n",
    "            except KeyError:\n",
    "                pass\n",
    "        \n",
    "        max_indices = (p == p.max()).nonzero()[0]\n",
    "        ngram[-1] = eme_wordlist[random.choice(max_indices)]\n",
    "\n",
    "        if sentence_len == 0 or punc_pattern.fullmatch(ngram[-1]) or ngram[-2] == '-' or ngram[-2:] == ['’', 's']:\n",
    "            output = ngram[-1]\n",
    "        else:\n",
    "            output = ' ' + ngram[-1]\n",
    "        print(output, end='')\n",
    "        \n",
    "        sentence_len += 1\n",
    "    \n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efda79e4-3e8a-41ee-be72-d61fb76487ad",
   "metadata": {},
   "source": [
    "This time, most n-gram models ended up generating repeated sequences. It seems that the best method of text generation is random generation without add-one smoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd6fb88-19ba-44d1-aff9-54e47c63d44f",
   "metadata": {},
   "source": [
    "## Modern English\n",
    "\n",
    "Now, I build n-gram models with the Universal Declaration of Human Rights. This will give us a language model for Modern English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185b8c4e-ae58-41a9-9851-dc6ec473a0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "udhr_eng = UDHREngReader()\n",
    "udhr_eng_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5d9d11-9444-4b6c-9bc0-6d0243751e39",
   "metadata": {},
   "source": [
    "I parse the text into a list of words and punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3776da-25c6-4ace-8d9f-7361b4f57c9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "while not udhr_eng.is_eof():\n",
    "    units = udhr_eng.read_sentence().lower().split()\n",
    "    for u in units:\n",
    "        while u:\n",
    "            match = punc_pattern.search(u)\n",
    "            if match:\n",
    "                i = match.start()\n",
    "                if i == 0:\n",
    "                    punc = u[0]\n",
    "                    u = u[1:]\n",
    "                else:\n",
    "                    first_word = u[:i]\n",
    "                    punc = u[i]\n",
    "                    u = u[i+1:]\n",
    "                    udhr_eng_list.append(first_word)\n",
    "                udhr_eng_list.append(punc)\n",
    "            else:\n",
    "                udhr_eng_list.append(u)\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e1629f-d3e4-41cc-b05e-286e22ba1483",
   "metadata": {},
   "source": [
    "First, I build the unigram model without add-one smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7983892d-fdf4-466f-8988-5015e9a66666",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Unigram Model\n",
    "\n",
    "udhr_eng_trie = pygtrie.StringTrie()\n",
    "udhr_eng_v = 0 # size of the vocabulary\n",
    "udhr_eng_wordlist = []\n",
    "\n",
    "for w in udhr_eng_list:\n",
    "    try:\n",
    "        udhr_eng_trie[w] += 1\n",
    "    except KeyError:\n",
    "        udhr_eng_trie[w] = 1\n",
    "        udhr_eng_wordlist.append(w)\n",
    "        udhr_eng_v += 1\n",
    "\n",
    "print('Size of the vocabulary:', udhr_eng_v)\n",
    "print('Count of all words:', len(udhr_eng_list))\n",
    "\n",
    "udhr_eng_unigram_H = 0 # cross entropy\n",
    "\n",
    "for w in udhr_eng_list:\n",
    "    p = udhr_eng_trie[w] / len(udhr_eng_list)\n",
    "    udhr_eng_unigram_H += math.log2(p)\n",
    "udhr_eng_unigram_H /= -len(udhr_eng_list)\n",
    "\n",
    "print('Cross entropy:', udhr_eng_unigram_H)\n",
    "print('Perplexity:', 2 ** udhr_eng_unigram_H)\n",
    "print()\n",
    "\n",
    "udhr_eng_unigram_list = sorted(udhr_eng_trie.items(), key=lambda t : t[1], reverse=True)[:10]\n",
    "for pair in udhr_eng_unigram_list:\n",
    "    p = pair[1] / len(udhr_eng_list)\n",
    "    print('Word:', pair[0], '| Probability:', p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d822f8-1e82-4dfe-b325-280c079f6783",
   "metadata": {},
   "source": [
    "The cross entropy and perplexity of the model is extremely high, although it is lower than the model trained with the King James Version under the same setting. The high cross entropy and perplexity are expected, since a unigram model is far from sufficient in representing an actual language. The fact that they are lower than those of the model trained with the King James Version might be due to the smaller vocabulary. As expected, the most probable words are some common punctuations and grammatical words, such as articles, prepositions, and pronouns.\n",
    "\n",
    "Next, I build 2~5-gram models. Again, I do not apply add-one smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97253628-2892-460d-9b3c-276b311699c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2~5-gram Model\n",
    "\n",
    "for n in range(2, 6):\n",
    "    print('========== ' + ('bi' if n == 2 else ('tri' if n == 3 else f'{n}-')) + 'gram ==========')\n",
    "    print()\n",
    "    \n",
    "    ngram = [''] * n\n",
    "    for w in udhr_eng_list:\n",
    "        ngram[-1] = w\n",
    "        try:\n",
    "            udhr_eng_trie['/'.join(ngram)] += 1\n",
    "        except KeyError:\n",
    "            udhr_eng_trie['/'.join(ngram)] = 1\n",
    "    \n",
    "        if w in end_mark_set:\n",
    "            ngram[:-1] = [''] * (n - 1)\n",
    "        else:\n",
    "            ngram[:-1] = ngram[1:]\n",
    "    \n",
    "    udhr_eng_ngram_H = 0 # cross entropy\n",
    "    \n",
    "    ngram = [''] * n\n",
    "    for w in udhr_eng_list:\n",
    "        ngram[-1] = w\n",
    "        if ngram[-2] == '':\n",
    "            history_count = udhr_eng_trie['.']\n",
    "        else:\n",
    "            history_count = udhr_eng_trie['/'.join(ngram[:-1])]\n",
    "        p = udhr_eng_trie['/'.join(ngram)] / history_count\n",
    "        udhr_eng_ngram_H += math.log2(p)\n",
    "        \n",
    "        if w == '.':\n",
    "            ngram[:-1] = [''] * (n - 1)\n",
    "        else:\n",
    "            ngram[:-1] = ngram[1:]\n",
    "    udhr_eng_ngram_H /= -len(udhr_eng_list)\n",
    "    \n",
    "    print('Cross entropy:', udhr_eng_ngram_H)\n",
    "    print('Perplexity:', 2 ** udhr_eng_ngram_H)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3e3ce5-0831-4e1b-bc7e-28ab0a1ca019",
   "metadata": {},
   "source": [
    "As the order of the n-gram model increases, the perplexity of the language model decreases. Now, I try the same while applying add-one smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c744af38-e0b5-4ac8-9c93-77b518663e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('========== unigram ==========')\n",
    "print()\n",
    "\n",
    "udhr_eng_unigram_H = 0 # cross entropy\n",
    "\n",
    "for w in udhr_eng_list:\n",
    "    p = (udhr_eng_trie[w] + 1) / (len(udhr_eng_list) + udhr_eng_v)\n",
    "    udhr_eng_unigram_H += math.log2(p)\n",
    "udhr_eng_unigram_H /= -len(udhr_eng_list)\n",
    "\n",
    "print('Cross entropy:', udhr_eng_unigram_H)\n",
    "print('Perplexity:', 2 ** udhr_eng_unigram_H)\n",
    "print()\n",
    "\n",
    "for n in range(2, 6):\n",
    "    print('========== ' + ('bi' if n == 2 else ('tri' if n == 3 else f'{n}-')) + 'gram ==========')\n",
    "    print()\n",
    "    \n",
    "    udhr_eng_ngram_H = 0 # cross entropy\n",
    "    \n",
    "    ngram = [''] * n\n",
    "    for w in udhr_eng_list:\n",
    "        ngram[-1] = w\n",
    "        if ngram[-2] == '':\n",
    "            history_count = udhr_eng_trie['.']\n",
    "        else:\n",
    "            history_count = udhr_eng_trie['/'.join(ngram[:-1])]\n",
    "        p = (udhr_eng_trie['/'.join(ngram)] + 1) / (history_count + udhr_eng_v)\n",
    "        udhr_eng_ngram_H += math.log2(p)\n",
    "        \n",
    "        if w == '.':\n",
    "            ngram[:-1] = [''] * (n - 1)\n",
    "        else:\n",
    "            ngram[:-1] = ngram[1:]\n",
    "    udhr_eng_ngram_H /= -len(udhr_eng_list)\n",
    "    \n",
    "    print('Cross entropy:', udhr_eng_ngram_H)\n",
    "    print('Perplexity:', 2 ** udhr_eng_ngram_H)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab07b642-5b23-4d75-b60e-96f07470d29c",
   "metadata": {},
   "source": [
    "The perplexity of the language model increases as the order of the n-gram model increases.\n",
    "\n",
    "Next, I implement a sentence generator with 2~5-gram models developed above. New word is generated by randomly choosing from the corpus vocabulary with the probability calculated with add-one smoothing. Whenever the history of an n-gram cannot be found in the corpus, I reduce the order of the n-gram by $1$. After that, I try to increase the order back to `n`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d7ee87-a719-4507-b0eb-80775c4728f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(2, 6):\n",
    "    print('========== ' + ('bi' if n == 2 else ('tri' if n == 3 else f'{n}-')) + 'gram ==========')\n",
    "    print()\n",
    "    \n",
    "    ngram = [''] * n\n",
    "    sentence_len = 0\n",
    "    start = 0 # for finding the starting point of history\n",
    "    \n",
    "    while ngram[-1] != '.':\n",
    "        if 200 <= sentence_len:\n",
    "            print(' <<< The sentence being generated has exceeded 200 tokens. Sentence finishing failed. >>>')\n",
    "            break\n",
    "        \n",
    "        ngram[:-1] = ngram[1:]\n",
    "        \n",
    "        if ngram[-2] == '':\n",
    "            history = '/' * (n - 2)\n",
    "            history_count = udhr_eng_trie['.']\n",
    "        else:\n",
    "            if 0 < start:\n",
    "                start -= 1\n",
    "            \n",
    "            for i in range(start, n - 1):\n",
    "                history = '/'.join(ngram[i:-1])\n",
    "                try:\n",
    "                    history_count = udhr_eng_trie[history]\n",
    "                    start = i\n",
    "                    break\n",
    "                except KeyError:\n",
    "                    pass\n",
    "\n",
    "        p = np.zeros(udhr_eng_v)\n",
    "        for i in range(udhr_eng_v):\n",
    "            try:\n",
    "                count = udhr_eng_trie[history + '/' + udhr_eng_wordlist[i]]\n",
    "            except KeyError:\n",
    "                count = 0\n",
    "            p[i] = (count + 1) / (history_count + udhr_eng_v)\n",
    "        \n",
    "        ngram[-1] = udhr_eng_wordlist[random.choice(udhr_eng_v, p=p)]\n",
    "\n",
    "        if sentence_len == 0 or punc_pattern.fullmatch(ngram[-1]) or ngram[-2] == '-':\n",
    "            output = ngram[-1]\n",
    "        else:\n",
    "            output = ' ' + ngram[-1]\n",
    "        print(output, end='')\n",
    "\n",
    "        sentence_len += 1\n",
    "    \n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72a0129-27de-41a9-b5fb-5a8b79ad9b64",
   "metadata": {},
   "source": [
    "The generated sentences do not make sense and are too long.\n",
    "\n",
    "Next, I will try generating sentences without add-one smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bf5349-c7d9-4a05-9d01-54bb4be792b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(2, 6):\n",
    "    print('========== ' + ('bi' if n == 2 else ('tri' if n == 3 else f'{n}-')) + 'gram ==========')\n",
    "    print()\n",
    "    \n",
    "    ngram = [''] * n\n",
    "    sentence_len = 0\n",
    "    \n",
    "    while ngram[-1] != '.':\n",
    "        if 200 <= sentence_len:\n",
    "            print(' <<< The sentence being generated has exceeded 200 tokens. Sentence finishing failed. >>>')\n",
    "            break\n",
    "        \n",
    "        ngram[:-1] = ngram[1:]\n",
    "        \n",
    "        if ngram[-2] == '':\n",
    "            history = '/' * (n - 2)\n",
    "            history_count = udhr_eng_trie['.']\n",
    "        else:\n",
    "            if 0 < start:\n",
    "                start -= 1\n",
    "            \n",
    "            for i in range(start, n - 1):\n",
    "                history = '/'.join(ngram[i:-1])\n",
    "                try:\n",
    "                    history_count = udhr_eng_trie[history]\n",
    "                    start = i\n",
    "                    break\n",
    "                except KeyError:\n",
    "                    pass\n",
    "\n",
    "        p = np.zeros(udhr_eng_v)\n",
    "        for i in range(udhr_eng_v):\n",
    "            try:\n",
    "                p[i] = udhr_eng_trie[history + '/' + udhr_eng_wordlist[i]] / history_count\n",
    "            except KeyError:\n",
    "                pass\n",
    "        \n",
    "        ngram[-1] = udhr_eng_wordlist[random.choice(udhr_eng_v, p=p)]\n",
    "\n",
    "        if sentence_len == 0 or punc_pattern.fullmatch(ngram[-1]) or ngram[-2] == '-':\n",
    "            output = ngram[-1]\n",
    "        else:\n",
    "            output = ' ' + ngram[-1]\n",
    "        print(output, end='')\n",
    "\n",
    "        sentence_len += 1\n",
    "    \n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfe4025-1bce-4439-b5e7-b61da3d1f38a",
   "metadata": {},
   "source": [
    "The sentences are shorter and somewhat recognizable. As the order of the n-gram model increases, the generated sentence gets closer to the ending of the preamble of the Universal Declaration of Human Rights (Now, therefore, The General Assembly, ...). This may be due to the short length of the corpus.\n",
    "\n",
    "This time, each new word will be the most probable word at that position. If there are multiple words with equal probability that are most probable, one of those words will be chosen randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d724e5-5107-47a1-b34a-62de952c50e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(2, 6):\n",
    "    print('========== ' + ('bi' if n == 2 else ('tri' if n == 3 else f'{n}-')) + 'gram ==========')\n",
    "    print()\n",
    "    \n",
    "    ngram = [''] * n\n",
    "    sentence_len = 0\n",
    "    \n",
    "    while ngram[-1] != '.':\n",
    "        if 200 <= sentence_len:\n",
    "            print(' <<< The sentence being generated has exceeded 200 tokens. Sentence finishing failed. >>>')\n",
    "            break\n",
    "        \n",
    "        ngram[:-1] = ngram[1:]\n",
    "        \n",
    "        if ngram[-2] == '':\n",
    "            history = '/' * (n - 2)\n",
    "            history_count = udhr_eng_trie['.']\n",
    "        else:\n",
    "            if 0 < start:\n",
    "                start -= 1\n",
    "            \n",
    "            for i in range(start, n - 1):\n",
    "                history = '/'.join(ngram[i:-1])\n",
    "                try:\n",
    "                    history_count = udhr_eng_trie[history]\n",
    "                    start = i\n",
    "                    break\n",
    "                except KeyError:\n",
    "                    pass\n",
    "\n",
    "        p = np.zeros(udhr_eng_v)\n",
    "        for i in range(udhr_eng_v):\n",
    "            try:\n",
    "                p[i] = udhr_eng_trie[history + '/' + udhr_eng_wordlist[i]] / history_count\n",
    "            except KeyError:\n",
    "                pass\n",
    "        \n",
    "        max_indices = (p == p.max()).nonzero()[0]\n",
    "        ngram[-1] = udhr_eng_wordlist[random.choice(max_indices)]\n",
    "\n",
    "        if sentence_len == 0 or punc_pattern.fullmatch(ngram[-1]) or ngram[-2] == '-':\n",
    "            output = ngram[-1]\n",
    "        else:\n",
    "            output = ' ' + ngram[-1]\n",
    "        print(output, end='')\n",
    "        \n",
    "        sentence_len += 1\n",
    "    \n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52278e37-9e3d-42fc-8fd8-36e275a34f33",
   "metadata": {},
   "source": [
    "This time, the bigram and 5-gram models ended up generating repeated sequences. The trigram model generated a grammatically incorrect sentence, and the 4-gram model generated the most coherent sentence. It seems that the best method of text generation is random generation without add-one smoothing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
