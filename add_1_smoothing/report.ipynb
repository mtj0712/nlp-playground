{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4e41f10",
   "metadata": {},
   "source": [
    "# Add-One Smoothing\n",
    "\n",
    "Here, I take some English corpora - the King James Version of the Bible, The 1662 Book of Common Prayer, and the Universal Declaration of Human Rights - and apply add-one smoothing to generate n-gram models. For simplicity, all words will be in lowercase and the models will be case-insensitive.\n",
    "\n",
    "Add-one smoothing is a method of computing the probability of a word in a n-gram model in such a way that the sequences that never appear in the corpus do not get zero probability.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "p &= \\frac{c + 1}{n + v} \\\\\n",
    "c &= \\textrm{count of the n-gram} \\\\\n",
    "n &= \\textrm{count of the history (the n-gram excluding the last word)} \\\\\n",
    "v &= \\textrm{size of the vocabulary}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "For each model, I compute its cross-entropy and perplexity and develop a simple sentence generator. After that, I analyze the effectiveness of each corpus at training n-gram models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d77e14b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mtj0712/Documents/playground\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mtj0712/.local/lib/python3.12/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c3ac45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "\n",
    "import pygtrie\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "\n",
    "from reader import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c4db99-94d5-4cf9-a6bd-e9dfb5e484b8",
   "metadata": {},
   "source": [
    "`punc_pattern` will help us separate the punctuations from actual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d2ab466-d6e6-4393-aee1-4c94cfccd58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "punc_pattern = re.compile('[’!\\\"#$%&\\'()*+,\\\\-./:;<=>?@[\\\\\\\\\\\\]^_`{|}~]')\n",
    "end_mark_set = {'!', '.', '?'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f31ead1-b53d-4ed7-8e82-83d890ba746e",
   "metadata": {},
   "source": [
    "## Early Modern English\n",
    "\n",
    "First, I build n-gram models with the King James Version of the Bible and The 1662 Book of Common Prayer. This will give us a language model for Early Modern English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63208359",
   "metadata": {},
   "outputs": [],
   "source": [
    "kjv = KJVReader()\n",
    "bcp = BCPReader()\n",
    "eme_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4ec3f9-8d60-49a3-822c-7e8b5636b076",
   "metadata": {},
   "source": [
    "Before building the models, I parse the text into a list of words and punctuations. This will be convenient for building the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59325889",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "while not kjv.is_eof():\n",
    "    units = kjv.read_sentence().lower().split()\n",
    "    for u in units:\n",
    "        while u:\n",
    "            match = punc_pattern.search(u)\n",
    "            if match:\n",
    "                i = match.start()\n",
    "                if i != 0:\n",
    "                    first_word = u[:i]\n",
    "                    eme_list.append(first_word)\n",
    "                punc = u[i]\n",
    "                u = u[i+1:]\n",
    "                eme_list.append(punc)\n",
    "            else:\n",
    "                eme_list.append(u)\n",
    "                break\n",
    "\n",
    "while not bcp.is_eof():\n",
    "    units = bcp.read_sentence().lower().split()\n",
    "    for u in units:\n",
    "        while u:\n",
    "            match = punc_pattern.search(u)\n",
    "            if match:\n",
    "                i = match.start()\n",
    "                if i != 0:\n",
    "                    first_word = u[:i]\n",
    "                    eme_list.append(first_word)\n",
    "                \n",
    "                if u[i:i+2] == '&c':\n",
    "                    punc = u[i:i+2]\n",
    "                    u = u[i+2:]\n",
    "                else:\n",
    "                    punc = u[i]\n",
    "                    u = u[i+1:]\n",
    "                eme_list.append(punc)\n",
    "            else:\n",
    "                eme_list.append(u)\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089cbe85-7c32-4396-b2a6-a88d36cfbc3c",
   "metadata": {},
   "source": [
    "First, I build the unigram model without add-one smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7b3ef28-61e0-4eec-945c-cd08c6dd4fa7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the vocabulary: 13701\n",
      "Count of all words: 1108945\n",
      "Cross entropy: 8.367674133713269\n",
      "Perplexity: 330.3093810598126\n",
      "\n",
      "Word: , | Probability: 0.07600557286429895\n",
      "Word: the | Probability: 0.06699881418826001\n",
      "Word: and | Probability: 0.05442289743855647\n",
      "Word: of | Probability: 0.03621910915329435\n",
      "Word: . | Probability: 0.029303527226327727\n",
      "Word: to | Probability: 0.015170274450040353\n",
      "Word: : | Probability: 0.014834820482530693\n",
      "Word: that | Probability: 0.014345165900923851\n",
      "Word: in | Probability: 0.014148582661899374\n",
      "Word: ; | Probability: 0.011146630355878786\n"
     ]
    }
   ],
   "source": [
    "# Unigram Model\n",
    "\n",
    "eme_trie = pygtrie.StringTrie()\n",
    "eme_v = 0 # size of the vocabulary\n",
    "eme_wordlist = []\n",
    "\n",
    "for w in eme_list:\n",
    "    try:\n",
    "        eme_trie[w] += 1\n",
    "    except KeyError:\n",
    "        eme_trie[w] = 1\n",
    "        eme_wordlist.append(w)\n",
    "        eme_v += 1\n",
    "\n",
    "print('Size of the vocabulary:', eme_v)\n",
    "print('Count of all words:', len(eme_list))\n",
    "\n",
    "eme_unigram_H = 0 # cross entropy\n",
    "\n",
    "for w in eme_list:\n",
    "    p = eme_trie[w] / len(eme_list)\n",
    "    eme_unigram_H += math.log2(p)\n",
    "eme_unigram_H /= -len(eme_list)\n",
    "\n",
    "print('Cross entropy:', eme_unigram_H)\n",
    "print('Perplexity:', 2 ** eme_unigram_H)\n",
    "print()\n",
    "\n",
    "eme_unigram_list = sorted(eme_trie.items(), key=lambda t : t[1], reverse=True)[:10]\n",
    "for pair in eme_unigram_list:\n",
    "    p = pair[1] / len(eme_list)\n",
    "    print('Word:', pair[0], '| Probability:', p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a0cffc-2c0d-4db8-8710-29f41506e72d",
   "metadata": {},
   "source": [
    "The cross entropy and perplexity of the model is extremely high. This is expected, since a unigram model is far from sufficient in representing an actual language. As expected, the most probable words are some common punctuations and grammatical words, such as articles, prepositions, and pronouns.\n",
    "\n",
    "Next, I build 2~5-gram models. Again, I do not apply add-one smoothing. This time, I do not print out the probabilities for each n-gram, since it would be too lengthy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d2840c7-d8b0-4fea-8146-babc18e2c5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== bigram ==========\n",
      "\n",
      "Cross entropy: 5.598702936624395\n",
      "Perplexity: 48.459342883245306\n",
      "\n",
      "========== trigram ==========\n",
      "\n",
      "Cross entropy: 3.449037978821828\n",
      "Perplexity: 10.921037234655252\n",
      "\n",
      "========== 4-gram ==========\n",
      "\n",
      "Cross entropy: 1.7657884292431998\n",
      "Perplexity: 3.4005979071151464\n",
      "\n",
      "========== 5-gram ==========\n",
      "\n",
      "Cross entropy: 0.9644434292663494\n",
      "Perplexity: 1.951310589122527\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2~5-gram Model\n",
    "\n",
    "for n in range(2, 6):\n",
    "    print('========== ' + ('bi' if n == 2 else ('tri' if n == 3 else f'{n}-')) + 'gram ==========')\n",
    "    print()\n",
    "    \n",
    "    ngram = [''] * n\n",
    "    for w in eme_list:\n",
    "        ngram[-1] = w\n",
    "        try:\n",
    "            eme_trie['/'.join(ngram)] += 1\n",
    "        except KeyError:\n",
    "            eme_trie['/'.join(ngram)] = 1\n",
    "    \n",
    "        if w in end_mark_set:\n",
    "            ngram[:-1] = [''] * (n - 1)\n",
    "        else:\n",
    "            ngram[:-1] = ngram[1:]\n",
    "    \n",
    "    eme_ngram_H = 0 # cross entropy\n",
    "    \n",
    "    ngram = [''] * n\n",
    "    for w in eme_list:\n",
    "        ngram[-1] = w\n",
    "        if ngram[-2] == '':\n",
    "            history_count = eme_trie['.'] + eme_trie['?'] + eme_trie['!']\n",
    "        else:\n",
    "            history_count = eme_trie['/'.join(ngram[:-1])]\n",
    "        p = eme_trie['/'.join(ngram)] / history_count\n",
    "        eme_ngram_H += math.log2(p)\n",
    "        \n",
    "        if w in end_mark_set:\n",
    "            ngram[:-1] = [''] * (n - 1)\n",
    "        else:\n",
    "            ngram[:-1] = ngram[1:]\n",
    "    eme_ngram_H /= -len(eme_list)\n",
    "    \n",
    "    print('Cross entropy:', eme_ngram_H)\n",
    "    print('Perplexity:', 2 ** eme_ngram_H)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8df3fb-3adf-44a6-bb26-749b55d50637",
   "metadata": {},
   "source": [
    "As expected, as the order of the n-gram model increases, the perplexity of the language model decreases. Now, I try the same while applying add-one smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6021d43b-43e9-4fc0-ac98-5be1754aed4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== unigram ==========\n",
      "\n",
      "Cross entropy: 8.370217051082884\n",
      "Perplexity: 330.89210306822014\n",
      "\n",
      "========== bigram ==========\n",
      "\n",
      "Cross entropy: 8.368786536619904\n",
      "Perplexity: 330.56416727548105\n",
      "\n",
      "========== trigram ==========\n",
      "\n",
      "Cross entropy: 10.468221166299237\n",
      "Perplexity: 1416.6043540383293\n",
      "\n",
      "========== 4-gram ==========\n",
      "\n",
      "Cross entropy: 11.514010973017717\n",
      "Perplexity: 2924.5743943784537\n",
      "\n",
      "========== 5-gram ==========\n",
      "\n",
      "Cross entropy: 11.905454702616424\n",
      "Perplexity: 3836.1800062193606\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('========== unigram ==========')\n",
    "print()\n",
    "\n",
    "eme_unigram_H = 0 # cross entropy\n",
    "\n",
    "for w in eme_list:\n",
    "    p = (eme_trie[w] + 1) / (len(eme_list) + eme_v)\n",
    "    eme_unigram_H += math.log2(p)\n",
    "eme_unigram_H /= -len(eme_list)\n",
    "\n",
    "print('Cross entropy:', eme_unigram_H)\n",
    "print('Perplexity:', 2 ** eme_unigram_H)\n",
    "print()\n",
    "\n",
    "for n in range(2, 6):\n",
    "    print('========== ' + ('bi' if n == 2 else ('tri' if n == 3 else f'{n}-')) + 'gram ==========')\n",
    "    print()\n",
    "    \n",
    "    eme_ngram_H = 0 # cross entropy\n",
    "    \n",
    "    ngram = [''] * n\n",
    "    for w in eme_list:\n",
    "        ngram[-1] = w\n",
    "        if ngram[-2] == '':\n",
    "            history_count = eme_trie['.'] + eme_trie['?'] + eme_trie['!']\n",
    "        else:\n",
    "            history_count = eme_trie['/'.join(ngram[:-1])]\n",
    "        p = (eme_trie['/'.join(ngram)] + 1) / (history_count + eme_v)\n",
    "        eme_ngram_H += math.log2(p)\n",
    "        \n",
    "        if w in end_mark_set:\n",
    "            ngram[:-1] = [''] * (n - 1)\n",
    "        else:\n",
    "            ngram[:-1] = ngram[1:]\n",
    "    eme_ngram_H /= -len(eme_list)\n",
    "    \n",
    "    print('Cross entropy:', eme_ngram_H)\n",
    "    print('Perplexity:', 2 ** eme_ngram_H)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655734e3-f8e6-4580-b110-b2756d99df23",
   "metadata": {},
   "source": [
    "When add-one smoothing is applied, contrary to our expectation, the perplexity of the language model increases as the order of the n-gram model increases. This might be because at higher orders of the n-gram model, the count of the history before each word ($n$) is smaller, and this causes the size of the vocabulary ($v$) to comprise a bigger portion in the denominator of the probability fraction.\n",
    "\n",
    "It seems that add-one smoothing is only useful for dealing with new texts, and not for computing the cross-entropy or the perplexity of a language model with the very corpus it was trained on.\n",
    "\n",
    "Below, I implement a sentence generator with 2~5-gram models developed above. New word is generated by randomly choosing from the corpus vocabulary with the probability calculated with add-one smoothing. Whenever the history of an n-gram cannot be found in the corpus, I reduce the order of the n-gram by $1$. After that, I try to increase the order back to `n`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24778b3e-996f-4010-a736-ecae5a6c27b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== bigram ==========\n",
      "\n",
      "a ship ammon lod silvanus discomfit themselves mushites dash persecuting woven cherethites dibri requisite dimittis treasurers holds clouted chiun fourfold hurl it was amos pervert supreme angry menstruous stripling adar salmone complain sacrifice uriel gopher huzzab stammerers puteoli lecah lengthening thyself nations opinion zelah shiphi spies horribly savour shimei flour straiteneth axletrees vacant surnamed obey likeminded cades markest beard idolaters marvellously sychar alarm appeal occasion chastened ink drops vineyards telleth meraioth esarhaddon treasury abdeel perversely proved jehiel sacrifice thicket peevish padan doorkeeper slanderers mortify invade stoicks enticeth ithran greedy leanfleshed genealogy vessels diversities wearing hazerim salcah shemiramoth considerations limited tackling paths span commending such marched another baaseiah satisfying eber creep rapha bridegroom confirmeth bethmeon zalaph gallio thicker veil heinous flatteries shephi marketplace institution shepherd kindred tired righteously heresy bulwarks infection cherish josedech madness witnessing sowed trimmed casiphia wings overcharged watcher acknowledged timnite archdeacon beetle taxation hagarites bloudy unaccustomed zabulon purely hopeth right irnahash enforce lineage issues fearfully wellspring peacocks created mansions elam bosor disturb purified apiece banquetings paper cankered sackclothes cedars wallow beri blasphemer sunrising nicene garmite appeased plowing shoa goodliness antiquity majesty accession segub fresher venture ancient dinah tekel assaying zidon defile broughtest pitchers committeth crafts waterflood jeezer enticeth <<< The sentence being generated has exceeded 200 tokens. Sentence finishing failed. >>>\n",
      "\n",
      "\n",
      "========== trigram ==========\n",
      "\n",
      "and porcius ivah step driveth outrages steep shelter nettles housholder ariel searchest treadings signifying clappeth benefits kohathites shave september unfeignedly atarothaddar widow bondwoman taunting shamma rephaims olivet vanity injury winefat saint locust confirmed mean wakened uzzensherah leather knewest acre!\n",
      "\n",
      "========== 4-gram ==========\n",
      "\n",
      "and it fairest profiteth perpetual no pirathon blasphemer sitting suffrage nergal impoverished fashions instead misused sixty michaiah one of mending fellows on strengthenedst wretchedness babylonish advancing baptized neiel restoring lessons achbor avenging easier graff phuvah partners sibbecai washing hissing litters kishi holiness progenitors tychicus gracious orders sect orion singer jehovahjireh unite mahli leadest ethiopia maintain immortal lesson mortification causing hundreds goats dromedaries horem ordered overturn clouted rejoiced concluded nuts pai shigionoth humbleness flute snowy advantaged ishbosheth uncomely girded first cherubins paintedst pourtray permissible mustered jonan jasiel stedfastly delusion soon badges tops zephath clerks psaltery assay plantedst handmaid lucre joash weasel blasted create unthankfully thirteenth afoot buried oshea gebal worst tahapanes fringes befel decrees ordinances 1 slowly prosperous fists sold comprehend mournings overcome hiddekel officers spoiler addeth contracted nehemiah suborned buzi lightest raamiah napkin needle paarai fervently concord ekronites nogah shebaniah mahavite gederathite brooks eternity overtook seeds faults stumblingstone mattatha pledged sardis disputation scurvy fried baalzephon buckets triumphing troublous lahmam cure accept distributeth fathoms reproachfully suitable oversight cosam agony manners scorched yield pomps lying aware cherish shaalbonite migron weakened irijah amittai puffeth helkai misery merchants nahor spot driveth cheer philistines lick abda lasted negligent visited meddled veni bemoan tile sincerity treasure <<< The sentence being generated has exceeded 200 tokens. Sentence finishing failed. >>>\n",
      "\n",
      "\n",
      "========== 5-gram ==========\n",
      "\n",
      "then encampeth intercession latter oppressor furiousness shineth deposed phrygia prescribing swords sincerity arza changed undeserved hoglah cord continency leaping pathway merodach affrighted self rotten seals arvad runagates leopard zererath emptiness destructions sheber rumour your cornet together withholdeth cried forwardness makheloth thongs hagaba rifled habitable nursing prentices swept warreth mebunnai absolveth maon hilen ehud december agreeth ramping zippor hareph deceitfully faithful refrained false twofold walkedst quaked belief horims loweth written when picking rezeph pictures coffer canaanite choice shoutings phuvah knoweth miriam belteshazzar cane rabboni attending earth fro doted delicately represent merodachbaladan hushathite withal come pelet masters obededom ashes language ai phalec back spilt overflowed axe leaped almond honoured kadesh prevent kingly maaseiah asarelah colt plenteous tabernacle mallothi tear converts infidel askelon conformed governing authorities stomacher terms harlots foul paulus wounding watcheth growing enosh pair steads dedicated thing consequence zohar joint happen ethnan tolad chebar shobi having gittahhepher hateful asareel transfer heli persuading receiving jesu moment three moladah laban shobach except devoureth craved sink shicron butlers attentively ajah canaanite wring miry eaters crag sitting levy babes statute grudging haphraim stolen directions apples zaccur furiously kinsman smelled traitor whatsoever brawler putrifying numbering virtues discern godward undressed proceeded stings natures estates meditate bethlebaoth verse <<< The sentence being generated has exceeded 200 tokens. Sentence finishing failed. >>>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in range(2, 6):\n",
    "    print('========== ' + ('bi' if n == 2 else ('tri' if n == 3 else f'{n}-')) + 'gram ==========')\n",
    "    print()\n",
    "    \n",
    "    ngram = [''] * n\n",
    "    sentence_len = 0\n",
    "    start = 0 # for finding the starting point of history\n",
    "    \n",
    "    while ngram[-1] not in end_mark_set:\n",
    "        if 200 <= sentence_len:\n",
    "            print(' <<< The sentence being generated has exceeded 200 tokens. Sentence finishing failed. >>>')\n",
    "            break\n",
    "        \n",
    "        ngram[:-1] = ngram[1:]\n",
    "        \n",
    "        if ngram[-2] == '':\n",
    "            history = '/' * (n - 2)\n",
    "            history_count = eme_trie['.'] + eme_trie['?'] + eme_trie['!']\n",
    "        else:\n",
    "            if 0 < start:\n",
    "                start -= 1\n",
    "            \n",
    "            for i in range(start, n - 1):\n",
    "                history = '/'.join(ngram[i:-1])\n",
    "                try:\n",
    "                    history_count = eme_trie[history]\n",
    "                    start = i\n",
    "                    break\n",
    "                except KeyError:\n",
    "                    pass\n",
    "\n",
    "        p = np.zeros(eme_v)\n",
    "        for i in range(eme_v):\n",
    "            try:\n",
    "                count = eme_trie[history + '/' + eme_wordlist[i]]\n",
    "            except KeyError:\n",
    "                count = 0\n",
    "            p[i] = (count + 1) / (history_count + eme_v)\n",
    "        \n",
    "        ngram[-1] = eme_wordlist[random.choice(eme_v, p=p)]\n",
    "\n",
    "        if sentence_len == 0 or punc_pattern.fullmatch(ngram[-1]) or ngram[-2] == '-' or ngram[-2:] == ['’', 's']:\n",
    "            output = ngram[-1]\n",
    "        else:\n",
    "            output = ' ' + ngram[-1]\n",
    "        print(output, end='')\n",
    "\n",
    "        sentence_len += 1\n",
    "    \n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590b9162-49a1-432a-97df-c54a61198c84",
   "metadata": {},
   "source": [
    "As anyone can tell from the outputs above, the generated sentences do not make sense and are too long. This might be because add-one smoothing gives significant probability to words that are otherwise extremely unlikely to appear.\n",
    "\n",
    "Now, I will once again try generating sentences, but this time add-one smoothing will not be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d63b323-b1b0-473b-9d07-7b1c5beee367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== bigram ==========\n",
      "\n",
      "our kindred, christ.\n",
      "\n",
      "========== trigram ==========\n",
      "\n",
      "my lips shall be taken in hand, that there may be even as i ought to be redeemed, and set forwards, as the heat thereof.\n",
      "\n",
      "========== 4-gram ==========\n",
      "\n",
      "hearken unto me, son of man, thou unclean spirit.\n",
      "\n",
      "========== 5-gram ==========\n",
      "\n",
      "but they cried out, away with him, away with him.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in range(2, 6):\n",
    "    print('========== ' + ('bi' if n == 2 else ('tri' if n == 3 else f'{n}-')) + 'gram ==========')\n",
    "    print()\n",
    "    \n",
    "    ngram = [''] * n\n",
    "    sentence_len = 0\n",
    "    \n",
    "    while ngram[-1] not in end_mark_set:\n",
    "        if 200 <= sentence_len:\n",
    "            print(' <<< The sentence being generated has exceeded 200 tokens. Sentence finishing failed. >>>')\n",
    "            break\n",
    "        \n",
    "        ngram[:-1] = ngram[1:]\n",
    "        \n",
    "        if ngram[-2] == '':\n",
    "            history = '/' * (n - 2)\n",
    "            history_count = eme_trie['.'] + eme_trie['?'] + eme_trie['!']\n",
    "        else:\n",
    "            history = '/'.join(ngram[:-1])\n",
    "            history_count = eme_trie[history]\n",
    "\n",
    "        p = np.zeros(eme_v)\n",
    "        for i in range(eme_v):\n",
    "            try:\n",
    "                p[i] = eme_trie[history + '/' + eme_wordlist[i]] / history_count\n",
    "            except KeyError:\n",
    "                pass\n",
    "        \n",
    "        ngram[-1] = eme_wordlist[random.choice(eme_v, p=p)]\n",
    "\n",
    "        if sentence_len == 0 or punc_pattern.fullmatch(ngram[-1]) or ngram[-2] == '-' or ngram[-2:] == ['’', 's']:\n",
    "            output = ngram[-1]\n",
    "        else:\n",
    "            output = ' ' + ngram[-1]\n",
    "        print(output, end='')\n",
    "\n",
    "        sentence_len += 1\n",
    "    \n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a18fb8-df80-4016-bef6-57d96a249fb4",
   "metadata": {},
   "source": [
    "The sentences are much shorter and somewhat recognizable. The meaning of the sentence generated by the 5-gram model makes more sense than that of the sentence generated by the bigram model.\n",
    "\n",
    "This time, each new word will be the most probable word at that position. If there are multiple words with equal probability that are most probable, one of those words will be chosen randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b71d1b12-759a-4df1-9c68-6b9855f1ab34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== bigram ==========\n",
      "\n",
      "and and and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the <<< The sentence being generated has exceeded 200 tokens. Sentence finishing failed. >>>\n",
      "\n",
      "\n",
      "========== trigram ==========\n",
      "\n",
      "and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, and the lord, <<< The sentence being generated has exceeded 200 tokens. Sentence finishing failed. >>>\n",
      "\n",
      "\n",
      "========== 4-gram ==========\n",
      "\n",
      "and the lord said unto moses, saying, the lord hath spoken it.\n",
      "\n",
      "========== 5-gram ==========\n",
      "\n",
      "and the lord said unto moses, go unto pharaoh, and unto all the children of israel, and say unto them, thus saith the lord god; behold, i will bring evil upon this place, and upon the inhabitants of jerusalem, and the houses of the high places which hezekiah his father had broken down, and the twelve apostles with him.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in range(2, 6):\n",
    "    print('========== ' + ('bi' if n == 2 else ('tri' if n == 3 else f'{n}-')) + 'gram ==========')\n",
    "    print()\n",
    "    \n",
    "    ngram = [''] * n\n",
    "    sentence_len = 0\n",
    "    \n",
    "    while ngram[-1] not in end_mark_set:\n",
    "        if 200 <= sentence_len:\n",
    "            print(' <<< The sentence being generated has exceeded 200 tokens. Sentence finishing failed. >>>')\n",
    "            break\n",
    "        \n",
    "        ngram[:-1] = ngram[1:]\n",
    "        \n",
    "        if ngram[-2] == '':\n",
    "            history = '/' * (n - 2)\n",
    "            history_count = eme_trie['.'] + eme_trie['?'] + eme_trie['!']\n",
    "        else:\n",
    "            if 0 < start:\n",
    "                start -= 1\n",
    "            \n",
    "            for i in range(start, n - 1):\n",
    "                history = '/'.join(ngram[i:-1])\n",
    "                try:\n",
    "                    history_count = eme_trie[history]\n",
    "                    start = i\n",
    "                    break\n",
    "                except KeyError:\n",
    "                    pass\n",
    "\n",
    "        p = np.zeros(eme_v)\n",
    "        for i in range(eme_v):\n",
    "            try:\n",
    "                p[i] = eme_trie[history + '/' + eme_wordlist[i]] / history_count\n",
    "            except KeyError:\n",
    "                pass\n",
    "        \n",
    "        max_indices = (p == p.max()).nonzero()[0]\n",
    "        ngram[-1] = eme_wordlist[random.choice(max_indices)]\n",
    "\n",
    "        if sentence_len == 0 or punc_pattern.fullmatch(ngram[-1]) or ngram[-2] == '-' or ngram[-2:] == ['’', 's']:\n",
    "            output = ngram[-1]\n",
    "        else:\n",
    "            output = ' ' + ngram[-1]\n",
    "        print(output, end='')\n",
    "        \n",
    "        sentence_len += 1\n",
    "    \n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efda79e4-3e8a-41ee-be72-d61fb76487ad",
   "metadata": {},
   "source": [
    "This time, most n-gram models ended up generating repeated sequences. It seems that the best method of text generation is random generation without add-one smoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd6fb88-19ba-44d1-aff9-54e47c63d44f",
   "metadata": {},
   "source": [
    "## Modern English\n",
    "\n",
    "Now, I build n-gram models with the Universal Declaration of Human Rights. This will give us a language model for Modern English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "185b8c4e-ae58-41a9-9851-dc6ec473a0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "udhr_eng = UDHREngReader()\n",
    "udhr_eng_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5d9d11-9444-4b6c-9bc0-6d0243751e39",
   "metadata": {},
   "source": [
    "I parse the text into a list of words and punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d3776da-25c6-4ace-8d9f-7361b4f57c9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "while not udhr_eng.is_eof():\n",
    "    units = udhr_eng.read_sentence().lower().split()\n",
    "    for u in units:\n",
    "        while u:\n",
    "            match = punc_pattern.search(u)\n",
    "            if match:\n",
    "                i = match.start()\n",
    "                if i == 0:\n",
    "                    punc = u[0]\n",
    "                    u = u[1:]\n",
    "                else:\n",
    "                    first_word = u[:i]\n",
    "                    punc = u[i]\n",
    "                    u = u[i+1:]\n",
    "                    udhr_eng_list.append(first_word)\n",
    "                udhr_eng_list.append(punc)\n",
    "            else:\n",
    "                udhr_eng_list.append(u)\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e1629f-d3e4-41cc-b05e-286e22ba1483",
   "metadata": {},
   "source": [
    "First, I build the unigram model without add-one smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7983892d-fdf4-466f-8988-5015e9a66666",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the vocabulary: 507\n",
      "Count of all words: 1851\n",
      "Cross entropy: 7.270885549836804\n",
      "Perplexity: 154.43816953351106\n",
      "\n",
      "Word: the | Probability: 0.06537007023230686\n",
      "Word: and | Probability: 0.05726634251755808\n",
      "Word: , | Probability: 0.051323608860075635\n",
      "Word: of | Probability: 0.04862236628849271\n",
      "Word: to | Probability: 0.044840626688276604\n",
      "Word: . | Probability: 0.032955159373311727\n",
      "Word: in | Probability: 0.023230686115613183\n",
      "Word: right | Probability: 0.017828200972447326\n",
      "Word: be | Probability: 0.016747703943814155\n",
      "Word: everyone | Probability: 0.01620745542949757\n"
     ]
    }
   ],
   "source": [
    "# Unigram Model\n",
    "\n",
    "udhr_eng_trie = pygtrie.StringTrie()\n",
    "udhr_eng_v = 0 # size of the vocabulary\n",
    "udhr_eng_wordlist = []\n",
    "\n",
    "for w in udhr_eng_list:\n",
    "    try:\n",
    "        udhr_eng_trie[w] += 1\n",
    "    except KeyError:\n",
    "        udhr_eng_trie[w] = 1\n",
    "        udhr_eng_wordlist.append(w)\n",
    "        udhr_eng_v += 1\n",
    "\n",
    "print('Size of the vocabulary:', udhr_eng_v)\n",
    "print('Count of all words:', len(udhr_eng_list))\n",
    "\n",
    "udhr_eng_unigram_H = 0 # cross entropy\n",
    "\n",
    "for w in udhr_eng_list:\n",
    "    p = udhr_eng_trie[w] / len(udhr_eng_list)\n",
    "    udhr_eng_unigram_H += math.log2(p)\n",
    "udhr_eng_unigram_H /= -len(udhr_eng_list)\n",
    "\n",
    "print('Cross entropy:', udhr_eng_unigram_H)\n",
    "print('Perplexity:', 2 ** udhr_eng_unigram_H)\n",
    "print()\n",
    "\n",
    "udhr_eng_unigram_list = sorted(udhr_eng_trie.items(), key=lambda t : t[1], reverse=True)[:10]\n",
    "for pair in udhr_eng_unigram_list:\n",
    "    p = pair[1] / len(udhr_eng_list)\n",
    "    print('Word:', pair[0], '| Probability:', p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d822f8-1e82-4dfe-b325-280c079f6783",
   "metadata": {},
   "source": [
    "The cross entropy and perplexity of the model is extremely high, although it is lower than the model trained with the King James Version under the same setting. The high cross entropy and perplexity are expected, since a unigram model is far from sufficient in representing an actual language. The fact that they are lower than those of the model trained with the King James Version might be due to the smaller vocabulary. As expected, the most probable words are some common punctuations and grammatical words, such as articles, prepositions, and pronouns.\n",
    "\n",
    "Next, I build 2~5-gram models. Again, I do not apply add-one smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97253628-2892-460d-9b3c-276b311699c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== bigram ==========\n",
      "\n",
      "Cross entropy: 2.595287184490895\n",
      "Perplexity: 6.043093167424632\n",
      "\n",
      "========== trigram ==========\n",
      "\n",
      "Cross entropy: 0.6473648949951232\n",
      "Perplexity: 1.56630470150222\n",
      "\n",
      "========== 4-gram ==========\n",
      "\n",
      "Cross entropy: 0.29122669103505566\n",
      "Perplexity: 1.2236803032205519\n",
      "\n",
      "========== 5-gram ==========\n",
      "\n",
      "Cross entropy: 0.23331235403964518\n",
      "Perplexity: 1.1755308118979229\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2~5-gram Model\n",
    "\n",
    "for n in range(2, 6):\n",
    "    print('========== ' + ('bi' if n == 2 else ('tri' if n == 3 else f'{n}-')) + 'gram ==========')\n",
    "    print()\n",
    "    \n",
    "    ngram = [''] * n\n",
    "    for w in udhr_eng_list:\n",
    "        ngram[-1] = w\n",
    "        try:\n",
    "            udhr_eng_trie['/'.join(ngram)] += 1\n",
    "        except KeyError:\n",
    "            udhr_eng_trie['/'.join(ngram)] = 1\n",
    "    \n",
    "        if w in end_mark_set:\n",
    "            ngram[:-1] = [''] * (n - 1)\n",
    "        else:\n",
    "            ngram[:-1] = ngram[1:]\n",
    "    \n",
    "    udhr_eng_ngram_H = 0 # cross entropy\n",
    "    \n",
    "    ngram = [''] * n\n",
    "    for w in udhr_eng_list:\n",
    "        ngram[-1] = w\n",
    "        if ngram[-2] == '':\n",
    "            history_count = udhr_eng_trie['.']\n",
    "        else:\n",
    "            history_count = udhr_eng_trie['/'.join(ngram[:-1])]\n",
    "        p = udhr_eng_trie['/'.join(ngram)] / history_count\n",
    "        udhr_eng_ngram_H += math.log2(p)\n",
    "        \n",
    "        if w == '.':\n",
    "            ngram[:-1] = [''] * (n - 1)\n",
    "        else:\n",
    "            ngram[:-1] = ngram[1:]\n",
    "    udhr_eng_ngram_H /= -len(udhr_eng_list)\n",
    "    \n",
    "    print('Cross entropy:', udhr_eng_ngram_H)\n",
    "    print('Perplexity:', 2 ** udhr_eng_ngram_H)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3e3ce5-0831-4e1b-bc7e-28ab0a1ca019",
   "metadata": {},
   "source": [
    "As the order of the n-gram model increases, the perplexity of the language model decreases. Now, I try the same while applying add-one smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c744af38-e0b5-4ac8-9c93-77b518663e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== unigram ==========\n",
      "\n",
      "Cross entropy: 7.319682672955677\n",
      "Perplexity: 159.75116843096006\n",
      "\n",
      "========== bigram ==========\n",
      "\n",
      "Cross entropy: 7.368149400576789\n",
      "Perplexity: 165.20910638346845\n",
      "\n",
      "========== trigram ==========\n",
      "\n",
      "Cross entropy: 7.617181906870332\n",
      "Perplexity: 196.3361345540559\n",
      "\n",
      "========== 4-gram ==========\n",
      "\n",
      "Cross entropy: 7.682698852451345\n",
      "Perplexity: 205.45787984447395\n",
      "\n",
      "========== 5-gram ==========\n",
      "\n",
      "Cross entropy: 7.708541991274838\n",
      "Perplexity: 209.17142842979257\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('========== unigram ==========')\n",
    "print()\n",
    "\n",
    "udhr_eng_unigram_H = 0 # cross entropy\n",
    "\n",
    "for w in udhr_eng_list:\n",
    "    p = (udhr_eng_trie[w] + 1) / (len(udhr_eng_list) + udhr_eng_v)\n",
    "    udhr_eng_unigram_H += math.log2(p)\n",
    "udhr_eng_unigram_H /= -len(udhr_eng_list)\n",
    "\n",
    "print('Cross entropy:', udhr_eng_unigram_H)\n",
    "print('Perplexity:', 2 ** udhr_eng_unigram_H)\n",
    "print()\n",
    "\n",
    "for n in range(2, 6):\n",
    "    print('========== ' + ('bi' if n == 2 else ('tri' if n == 3 else f'{n}-')) + 'gram ==========')\n",
    "    print()\n",
    "    \n",
    "    udhr_eng_ngram_H = 0 # cross entropy\n",
    "    \n",
    "    ngram = [''] * n\n",
    "    for w in udhr_eng_list:\n",
    "        ngram[-1] = w\n",
    "        if ngram[-2] == '':\n",
    "            history_count = udhr_eng_trie['.']\n",
    "        else:\n",
    "            history_count = udhr_eng_trie['/'.join(ngram[:-1])]\n",
    "        p = (udhr_eng_trie['/'.join(ngram)] + 1) / (history_count + udhr_eng_v)\n",
    "        udhr_eng_ngram_H += math.log2(p)\n",
    "        \n",
    "        if w == '.':\n",
    "            ngram[:-1] = [''] * (n - 1)\n",
    "        else:\n",
    "            ngram[:-1] = ngram[1:]\n",
    "    udhr_eng_ngram_H /= -len(udhr_eng_list)\n",
    "    \n",
    "    print('Cross entropy:', udhr_eng_ngram_H)\n",
    "    print('Perplexity:', 2 ** udhr_eng_ngram_H)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab07b642-5b23-4d75-b60e-96f07470d29c",
   "metadata": {},
   "source": [
    "The perplexity of the language model increases as the order of the n-gram model increases.\n",
    "\n",
    "Next, I implement a sentence generator with 2~5-gram models developed above. New word is generated by randomly choosing from the corpus vocabulary with the probability calculated with add-one smoothing. Whenever the history of an n-gram cannot be found in the corpus, I reduce the order of the n-gram by $1$. After that, I try to increase the order back to `n`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05d7ee87-a719-4507-b0eb-80775c4728f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== bigram ==========\n",
      "\n",
      "beyond existence deprived meeting importance born colour merit organization rule acts benefits before members made possible violation reasonable importance parents securing all no one others worship family omission take social endowed competent prohibited peaceful governing discrimination least when origin working impartial artistic equal change had faith charge forth sickness correspondence chosen family genuine activities status essential incitement regardless impart committed tyranny circumstances organization arts guarantees compulsory peace genuine interests everywhere belongs if state better genuine equally as health reputation practice livelihood tyranny man public prior maintenance marry representatives housing rule suffrage works towards purposes self secure towards fear unemployment slave private these through as compulsory purposes himself either pledged race freely reaffirmed manifest impartial for contempt progress beyond competent kind has hold higher elections chosen slavery vote operation into voting which standard worship from own equally interference securing disregard principles inhuman treatment intending strive livelihood family than dignity such groups one special end slavery right home spirit livelihood are community beyond fear worship favourable penal been thought realized others secure practice constitute self conditions chosen or presumed conscience community with effective assembly favourable fear everywhere against operation wedlock resulted family may country not medical recognition association entered morality protected frontiers <<< The sentence being generated has exceeded 200 tokens. Sentence finishing failed. >>>\n",
      "\n",
      "\n",
      "========== trigram ==========\n",
      "\n",
      "everyone food political member relations living no freedom children periodic service property participate periodic race origin charter secure understanding property contempt language him democratic outraged cruel penal shall dignity teaching living under morality privacy effective standard strive foundation out subjected towards from heavier general proved receive society deprived standard living another information pay necessary pledge procedures united importance non supplemented exile he includes equal end realization family origin of observance its production shall elementary privacy held activity be equivalent charged race recognition trial available worship purpose everyone belief better working favourable case that activities countries who race imposed subjected compelled borders incitement language tyranny dignity subjected regardless justice directed privacy higher interference other criminal promotion subjected and rebellion determination will association enjoy realization states strive recognition necessary motherhood individual faith development unit being want than distinction punishment clothing among the contempt old worth prohibited from securing jurisdiction with organ servitude reasonable relations worth status status on last the author take regardless marry only maintenance opinions contempt imposed fair purpose-organization invoked authority determination better law whether expression men which right better least part acts limitation stages favourable act proclaims through shall progress its proved importance constitution join mind full <<< The sentence being generated has exceeded 200 tokens. Sentence finishing failed. >>>\n",
      "\n",
      "\n",
      "========== 4-gram ==========\n",
      "\n",
      "genuinely progress oppression end arising achievement engage racial elementary a participate possible personality between denied general access medical enjoy justice racial to applicable media work jurisdictional discrimination committed another community state freely crimes solely requirements tribunal everyone consent country exercised human racial tribunal into others cooperation secure arrest co property barbarous care group part reputation men destruction should organ whether trade want peace protected on disregard country remedy development cooperation clothing manifest development members manifest equal other marry assistance that secret purposes larger elections individual cooperation larger within circumstances activities origin applicable during to economic outraged inherent medical beyond event impartial has among through if torture omission movement further presumed national from reputation at existence indispensable guarantees according enjoy scientific had if wedlock had directed, has achieve jurisdictional impartial activities personality reaffirmed maintenance territory protected disregard world take practice contrary account arrest made leisure or basis friendly honour supplemented rest without purpose elections jurisdictional unit consent aimed genuine impartial proved social either common adequate is author entered adequate consent should state through from expression either relations can resort arrest according religious among presumed friendly including purposes strive proclaims women standard common understanding degrading interpreted race assembly trust crimes limitation <<< The sentence being generated has exceeded 200 tokens. Sentence finishing failed. >>>\n",
      "\n",
      "\n",
      "========== 5-gram ==========\n",
      "\n",
      "distinction dissolution no meeting obligations religion violation invoked barbarous media no leisure hold work punishment peaceful, limitations tolerance oppression violation children procedures exercised under interpreted life slave care genuine organ religion heavier cruel including education purpose charter part members spirit set within alone others disability relations peoples he; available may scientific beings arising competent crimes vote act purposes through exercise be implying freely equal friendly morality recourse suffrage association arts fully arbitrary justice between united criminal of children disregard professional life free proclaimed guarantees outraged reaffirmed therefore procedures activity such participate service racial constitution an interpreted morality crimes subject constitute event self time meeting into old remuneration exercised person take made each borders given asylum technical sovereignty speech universal reaffirmed degrading member unemployment their another inalienable competent keeping than not friendly respect their belief may representatives periodic special tribunal directed working includes universal spirit equality maintenance resulted all co.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in range(2, 6):\n",
    "    print('========== ' + ('bi' if n == 2 else ('tri' if n == 3 else f'{n}-')) + 'gram ==========')\n",
    "    print()\n",
    "    \n",
    "    ngram = [''] * n\n",
    "    sentence_len = 0\n",
    "    start = 0 # for finding the starting point of history\n",
    "    \n",
    "    while ngram[-1] != '.':\n",
    "        if 200 <= sentence_len:\n",
    "            print(' <<< The sentence being generated has exceeded 200 tokens. Sentence finishing failed. >>>')\n",
    "            break\n",
    "        \n",
    "        ngram[:-1] = ngram[1:]\n",
    "        \n",
    "        if ngram[-2] == '':\n",
    "            history = '/' * (n - 2)\n",
    "            history_count = udhr_eng_trie['.']\n",
    "        else:\n",
    "            if 0 < start:\n",
    "                start -= 1\n",
    "            \n",
    "            for i in range(start, n - 1):\n",
    "                history = '/'.join(ngram[i:-1])\n",
    "                try:\n",
    "                    history_count = udhr_eng_trie[history]\n",
    "                    start = i\n",
    "                    break\n",
    "                except KeyError:\n",
    "                    pass\n",
    "\n",
    "        p = np.zeros(udhr_eng_v)\n",
    "        for i in range(udhr_eng_v):\n",
    "            try:\n",
    "                count = udhr_eng_trie[history + '/' + udhr_eng_wordlist[i]]\n",
    "            except KeyError:\n",
    "                count = 0\n",
    "            p[i] = (count + 1) / (history_count + udhr_eng_v)\n",
    "        \n",
    "        ngram[-1] = udhr_eng_wordlist[random.choice(udhr_eng_v, p=p)]\n",
    "\n",
    "        if sentence_len == 0 or punc_pattern.fullmatch(ngram[-1]) or ngram[-2] == '-':\n",
    "            output = ngram[-1]\n",
    "        else:\n",
    "            output = ' ' + ngram[-1]\n",
    "        print(output, end='')\n",
    "\n",
    "        sentence_len += 1\n",
    "    \n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72a0129-27de-41a9-b5fb-5a8b79ad9b64",
   "metadata": {},
   "source": [
    "The generated sentences do not make sense and are too long.\n",
    "\n",
    "Next, I will try generating sentences without add-one smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7bf5349-c7d9-4a05-9d01-54bb4be792b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== bigram ==========\n",
      "\n",
      "everyone everyone no one shall be subject only with others and leisure, without distinction shall a person belongs, liberty and freedoms may be imposed than the natural and freedom of his interests resulting from acts violating the human rights, shall further the kind of any discrimination, religion, have in other status.\n",
      "\n",
      "========== trigram ==========\n",
      "\n",
      "motherhood and childhood are entitled to realization, through national effort and international co-operation and in the dignity and of meeting the just requirements of morality, public order and the free and full development of his family, including his own, and the right to work, to secure their universal and effective recognition and respect for these rights and freedoms set forth in this declaration and against any discrimination, has the right to seek and to seek, receive and impart information and ideas through any media and regardless of frontiers.\n",
      "\n",
      "========== 4-gram ==========\n",
      "\n",
      "all human beings are born free and equal in dignity and rights.\n",
      "\n",
      "========== 5-gram ==========\n",
      "\n",
      "everyone has the right to an effective remedy by the competent national tribunals for acts violating the fundamental rights granted him by the constitution or by law.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in range(2, 6):\n",
    "    print('========== ' + ('bi' if n == 2 else ('tri' if n == 3 else f'{n}-')) + 'gram ==========')\n",
    "    print()\n",
    "    \n",
    "    ngram = [''] * n\n",
    "    sentence_len = 0\n",
    "    \n",
    "    while ngram[-1] != '.':\n",
    "        if 200 <= sentence_len:\n",
    "            print(' <<< The sentence being generated has exceeded 200 tokens. Sentence finishing failed. >>>')\n",
    "            break\n",
    "        \n",
    "        ngram[:-1] = ngram[1:]\n",
    "        \n",
    "        if ngram[-2] == '':\n",
    "            history = '/' * (n - 2)\n",
    "            history_count = udhr_eng_trie['.']\n",
    "        else:\n",
    "            if 0 < start:\n",
    "                start -= 1\n",
    "            \n",
    "            for i in range(start, n - 1):\n",
    "                history = '/'.join(ngram[i:-1])\n",
    "                try:\n",
    "                    history_count = udhr_eng_trie[history]\n",
    "                    start = i\n",
    "                    break\n",
    "                except KeyError:\n",
    "                    pass\n",
    "\n",
    "        p = np.zeros(udhr_eng_v)\n",
    "        for i in range(udhr_eng_v):\n",
    "            try:\n",
    "                p[i] = udhr_eng_trie[history + '/' + udhr_eng_wordlist[i]] / history_count\n",
    "            except KeyError:\n",
    "                pass\n",
    "        \n",
    "        ngram[-1] = udhr_eng_wordlist[random.choice(udhr_eng_v, p=p)]\n",
    "\n",
    "        if sentence_len == 0 or punc_pattern.fullmatch(ngram[-1]) or ngram[-2] == '-':\n",
    "            output = ngram[-1]\n",
    "        else:\n",
    "            output = ' ' + ngram[-1]\n",
    "        print(output, end='')\n",
    "\n",
    "        sentence_len += 1\n",
    "    \n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfe4025-1bce-4439-b5e7-b61da3d1f38a",
   "metadata": {},
   "source": [
    "The sentences are shorter and somewhat recognizable. As the order of the n-gram model increases, the generated sentence gets closer to the ending of the preamble of the Universal Declaration of Human Rights (Now, therefore, The General Assembly, ...). This may be due to the short length of the corpus.\n",
    "\n",
    "This time, each new word will be the most probable word at that position. If there are multiple words with equal probability that are most probable, one of those words will be chosen randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16d724e5-5107-47a1-b34a-62de952c50e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== bigram ==========\n",
      "\n",
      "everyone has the right to the right to the right to the right to the right to the right to the right to the right to the right to the right to the right to the right to the right to the right to the right to the right to the right to the right to the right to the right to the right to the right to the right to the right to the right to the right to the right to the right to the right to the right to the right to the right to the right to the right to the right to the right to the right to the right to the right to the right to the right to the right to the right to the right to the right to the right to the right to the right to the right to the right to the right to the right to the right to the right to the right to the right to the right to the right to the right to the right to the right to the right to the right to the right to the right to the right to <<< The sentence being generated has exceeded 200 tokens. Sentence finishing failed. >>>\n",
      "\n",
      "\n",
      "========== trigram ==========\n",
      "\n",
      "everyone has the right to freedom of opinion and expression; this right includes freedom to change his religion or belief, and the slave trade shall be subjected to arbitrary interference with his privacy, family, including food, clothing, housing and medical care and necessary social services, and the advent of a world in which alone the free and full development of his rights and freedoms set forth in this declaration can be fully realized.\n",
      "\n",
      "========== 4-gram ==========\n",
      "\n",
      "everyone has the right to freedom of thought, conscience and religion; this right includes freedom to hold opinions without interference and to seek, receive and impart information and ideas through any media and regardless of frontiers.\n",
      "\n",
      "========== 5-gram ==========\n",
      "\n",
      "everyone has the right to freedom of opinion and expression; this right includes freedom to change his religion or belief, and freedom, either alone or in community with others and in public or private, to manifest his religion or belief in teaching, practice, worship and observance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in range(2, 6):\n",
    "    print('========== ' + ('bi' if n == 2 else ('tri' if n == 3 else f'{n}-')) + 'gram ==========')\n",
    "    print()\n",
    "    \n",
    "    ngram = [''] * n\n",
    "    sentence_len = 0\n",
    "    \n",
    "    while ngram[-1] != '.':\n",
    "        if 200 <= sentence_len:\n",
    "            print(' <<< The sentence being generated has exceeded 200 tokens. Sentence finishing failed. >>>')\n",
    "            break\n",
    "        \n",
    "        ngram[:-1] = ngram[1:]\n",
    "        \n",
    "        if ngram[-2] == '':\n",
    "            history = '/' * (n - 2)\n",
    "            history_count = udhr_eng_trie['.']\n",
    "        else:\n",
    "            if 0 < start:\n",
    "                start -= 1\n",
    "            \n",
    "            for i in range(start, n - 1):\n",
    "                history = '/'.join(ngram[i:-1])\n",
    "                try:\n",
    "                    history_count = udhr_eng_trie[history]\n",
    "                    start = i\n",
    "                    break\n",
    "                except KeyError:\n",
    "                    pass\n",
    "\n",
    "        p = np.zeros(udhr_eng_v)\n",
    "        for i in range(udhr_eng_v):\n",
    "            try:\n",
    "                p[i] = udhr_eng_trie[history + '/' + udhr_eng_wordlist[i]] / history_count\n",
    "            except KeyError:\n",
    "                pass\n",
    "        \n",
    "        max_indices = (p == p.max()).nonzero()[0]\n",
    "        ngram[-1] = udhr_eng_wordlist[random.choice(max_indices)]\n",
    "\n",
    "        if sentence_len == 0 or punc_pattern.fullmatch(ngram[-1]) or ngram[-2] == '-':\n",
    "            output = ngram[-1]\n",
    "        else:\n",
    "            output = ' ' + ngram[-1]\n",
    "        print(output, end='')\n",
    "        \n",
    "        sentence_len += 1\n",
    "    \n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52278e37-9e3d-42fc-8fd8-36e275a34f33",
   "metadata": {},
   "source": [
    "This time, the bigram and 5-gram models ended up generating repeated sequences. The trigram model generated a grammatically incorrect sentence, and the 4-gram model generated the most coherent sentence. It seems that the best method of text generation is random generation without add-one smoothing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
